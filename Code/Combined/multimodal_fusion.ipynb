{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T16:54:39.690831Z",
     "start_time": "2024-05-31T16:54:39.042250Z"
    }
   },
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import utility_functions as utils\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "custom_stop_words = list(STOP_WORDS)  # Existing stop words\n",
    "custom_stop_words.extend([\"ll\", \"ve\", \"'em\", \"em\", \"ho\", \"fo\", \"ah\", \"de\"])  # Tokens which doesn't really make sense to have them.\n",
    "\n",
    "best_model_settings = {\n",
    "    'lyrics_only': {\n",
    "        'random_forest':  {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200},\n",
    "        'gradient_boosting': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100},\n",
    "        'adaboost': {'learning_rate': 0.1, 'n_estimators': 100}\n",
    "    },\n",
    "    'hardness_audio_features': {\n",
    "        'random_forest': {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200},\n",
    "        'gradient_boosting': {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100},\n",
    "        'adaboost': {'learning_rate': 0.01, 'n_estimators': 100}\n",
    "    },\n",
    "    'hardness_and_lyrics_and_topic_model': {\n",
    "        'random_forest': {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200},\n",
    "        'gradient_boosting': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200},\n",
    "        'adaboost': {'learning_rate': 1.0, 'n_estimators': 100}\n",
    "    },\n",
    "    'audio_and_lyrics_and_emotions': {\n",
    "        'random_forest': {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "        'gradient_boosting': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100},\n",
    "        'adaboost': {'learning_rate': 1.0, 'n_estimators': 50}\n",
    "    },\n",
    "    'audio_and_lyrics_and_emotions_and_locations': {\n",
    "        'random_forest': {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200},\n",
    "        'gradient_boosting': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200},\n",
    "        'adaboost': {'learning_rate': 1.0, 'n_estimators': 50}\n",
    "    }\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data.xlsx', engine='openpyxl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T09:42:52.696502Z",
     "start_time": "2024-05-31T09:42:52.526022Z"
    }
   },
   "id": "1442741900b22ac8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def get_lyrics_embeddings(lyrics):\n",
    "    max_length = 512\n",
    "    tokens = tokenizer.tokenize(lyrics)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    all_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer.convert_tokens_to_ids(chunk)\n",
    "        inputs = tokenizer.prepare_for_model(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            all_embeddings.append(embeddings)\n",
    "    aggregated_embeddings = torch.mean(torch.stack(all_embeddings), dim=0)\n",
    "    return aggregated_embeddings.numpy()\n",
    "\n",
    "def get_audio_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    features = np.concatenate([mfcc.mean(axis=1), chroma.mean(axis=1), contrast.mean(axis=1)])\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T09:42:54.564309Z",
     "start_time": "2024-05-31T09:42:53.468610Z"
    }
   },
   "id": "a7adcaf6a23057a7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def run_ensemble_model(X, y, datatype, best_settings=None, save=False):\n",
    "    if best_settings is None:\n",
    "        param_grid_rf = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "\n",
    "        param_grid_gb = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "\n",
    "        param_grid_ada = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "\n",
    "        # Define classifiers\n",
    "        random_forest = RandomForestClassifier(random_state=42)\n",
    "        gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "        ada_boost = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "        # Perform grid search for each classifier\n",
    "        scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "        grid_search_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring=scorer, cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
    "        grid_search_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid_gb, scoring=scorer, cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
    "        grid_search_ada = GridSearchCV(estimator=ada_boost, param_grid=param_grid_ada, scoring=scorer, cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
    "\n",
    "        # Fit grid search\n",
    "        grid_search_rf.fit(X, y)\n",
    "        grid_search_gb.fit(X, y)\n",
    "        grid_search_ada.fit(X, y)\n",
    "\n",
    "        # Print best parameters for each model\n",
    "        print(f\"Best parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "        print(f\"Best parameters for Gradient Boosting: {grid_search_gb.best_params_}\")\n",
    "        print(f\"Best parameters for AdaBoost: {grid_search_ada.best_params_}\")\n",
    "\n",
    "        # Get best estimators\n",
    "        best_rf = grid_search_rf.best_estimator_\n",
    "        best_gb = grid_search_gb.best_estimator_\n",
    "        best_ada = grid_search_ada.best_estimator_\n",
    "    else:\n",
    "        # Load the best settings if provided\n",
    "        best_rf = RandomForestClassifier(random_state=42, **best_settings[datatype]['random_forest'])\n",
    "        best_gb = GradientBoostingClassifier(random_state=42, **best_settings[datatype]['gradient_boosting'])\n",
    "        best_ada = AdaBoostClassifier(random_state=42, **best_settings[datatype]['adaboost'])\n",
    "\n",
    "    if save:\n",
    "        joblib.dump(best_rf, f'best_random_forest_{datatype}.pkl')\n",
    "        joblib.dump(best_gb, f'best_gradient_boosting_{datatype}.pkl')\n",
    "        joblib.dump(best_ada, f'best_adaboost_{datatype}.pkl')\n",
    "\n",
    "    # Define an ensemble of the best estimators\n",
    "    voting_clf = VotingClassifier(estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('ada', best_ada)\n",
    "    ], voting='soft')\n",
    "\n",
    "    # List of classifiers to evaluate\n",
    "    classifiers = {\n",
    "        'Random Forest': best_rf,\n",
    "        'Gradient Boosting': best_gb,\n",
    "        'AdaBoost': best_ada,\n",
    "        'Voting Classifier': voting_clf\n",
    "    }\n",
    "\n",
    "    # Evaluate each classifier using cross-validation and collect metrics\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    metrics = {'accuracy': {}, 'precision': {}, 'recall': {}, 'f1': {}}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        accuracy = cross_val_score(clf, X, y, cv=skf, scoring='accuracy')\n",
    "        precision = cross_val_score(clf, X, y, cv=skf, scoring='precision_weighted')\n",
    "        recall = cross_val_score(clf, X, y, cv=skf, scoring='recall_weighted')\n",
    "        f1 = cross_val_score(clf, X, y, cv=skf, scoring='f1_weighted')\n",
    "\n",
    "        metrics['accuracy'][name] = accuracy\n",
    "        metrics['precision'][name] = precision\n",
    "        metrics['recall'][name] = recall\n",
    "        metrics['f1'][name] = f1\n",
    "\n",
    "        y_pred = cross_val_predict(clf, X, y, cv=skf)\n",
    "        report = classification_report(y, y_pred, target_names=label_encoder.classes_)\n",
    "        print(f\"Classifier: {name}\")\n",
    "        print(report)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return metrics  # Optionally return metrics if needed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:07:07.455168Z",
     "start_time": "2024-05-31T10:07:07.450675Z"
    }
   },
   "id": "cd8b3465214d7fd5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Compute features</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d76a2f0dd41266"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "tqdm.pandas(desc=\"Extracting lyrics embeddings\")\n",
    "df['Lyrics'] = df['Lyrics'].apply(utils.cleanup)\n",
    "df['lyrics_embeddings'] = df['Lyrics'].progress_apply(get_lyrics_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T09:52:14.015182Z",
     "start_time": "2024-05-31T09:43:13.441002Z"
    }
   },
   "id": "174c962a3f1096bb",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# tqdm.pandas(desc=\"Extracting audio features\")\n",
    "# df['audio_features'] = df['Path'].progress_apply(get_audio_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T21:32:26.525621Z",
     "start_time": "2024-05-27T20:16:52.268295Z"
    }
   },
   "id": "93748fa1637c58a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "# # Save to a pickle file\n",
    "# with open('dataframe.pkl', 'wb') as file:\n",
    "#     pickle.dump(df, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T09:59:43.140365Z",
     "start_time": "2024-05-31T09:59:43.116894Z"
    }
   },
   "id": "a9de9c492f1bd4a2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T09:59:43.625909Z",
     "start_time": "2024-05-31T09:59:43.612022Z"
    }
   },
   "id": "2913c8efc2e4ed33",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Lyrics only</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0eb39275182bf38"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "lyrics_features = np.vstack(df['lyrics_embeddings'].values)\n",
    "\n",
    "# Normalize lyrics embeddings\n",
    "scaler_lyrics = StandardScaler()\n",
    "X_lyrics = scaler_lyrics.fit_transform(lyrics_features)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_lyrics = df['coast_label'].values\n",
    "\n",
    "only_lyrics_metrics = run_ensemble_model(X_lyrics, y_lyrics, datatype='lyrics_only', best_settings=best_model_settings, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:22:16.307967Z",
     "start_time": "2024-05-31T10:08:20.765057Z"
    }
   },
   "id": "2a43a8dbdf9b73f",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Hardness audio</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f93a2982dca58f1"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data_w_topics_w_features.xlsx', engine='openpyxl')\n",
    "\n",
    "file_path = '/Users/borosabel/Documents/Uni/Thesis/PopMIR/Code/Lyrics/indexes.txt'\n",
    "\n",
    "# Read the file and convert each line to an integer\n",
    "with open(file_path, 'r') as file:\n",
    "    indices = [int(line.strip()) for line in file]\n",
    "    \n",
    "df = df.iloc[indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:25:19.792605Z",
     "start_time": "2024-05-31T10:25:16.586746Z"
    }
   },
   "id": "9ed34cb258e089cf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "columns_to_remove = ['Artist', 'Album', 'Song', 'Release Year', 'Tempo2', 'Duration (s)', 'Sample Rate (Hz)', 'Path', 'Lyrics', 'Coast']\n",
    "label_column = 'Coast'\n",
    "feature_columns = [item for item in list(df.columns) if item not in columns_to_remove]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:25:19.794621Z",
     "start_time": "2024-05-31T10:25:19.792952Z"
    }
   },
   "id": "4bb5a117c3547701",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "features = df[feature_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:25:21.270325Z",
     "start_time": "2024-05-31T10:25:21.262565Z"
    }
   },
   "id": "dc0e197596867dbe",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_hardness = df['coast_label'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_hardness = scaler.fit_transform(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:25:23.588852Z",
     "start_time": "2024-05-31T10:25:23.582540Z"
    }
   },
   "id": "364d5d6f4acdc35a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "hardness_audio_features_metrics = run_ensemble_model(X_hardness, y_hardness, datatype='hardness_audio_features', best_settings=best_model_settings, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:32:36.356796Z",
     "start_time": "2024-05-31T10:25:27.383494Z"
    }
   },
   "id": "25d4bec8a47588fb",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Hardness Features + Lyrics embeddings</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1dccea7344e4254"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data_w_topics_w_features.xlsx', engine='openpyxl')\n",
    "\n",
    "indices_file_path = '/Users/borosabel/Documents/Uni/Thesis/PopMIR/Code/Lyrics/indexes.txt'\n",
    "\n",
    "# Read the file and convert each line to an integer\n",
    "with open(file_path, 'r') as file:\n",
    "    indices = [int(line.strip()) for line in file]\n",
    "\n",
    "df = df.iloc[indices]\n",
    "\n",
    "columns_to_remove = ['Artist', 'Album', 'Song', 'Release Year', 'Tempo2', 'Duration (s)', 'Sample Rate (Hz)', 'Path', 'Lyrics', 'Coast']\n",
    "label_column = 'Coast'\n",
    "feature_columns = [item for item in list(df.columns) if item not in columns_to_remove]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:19.517661Z",
     "start_time": "2024-05-31T10:42:16.190159Z"
    }
   },
   "id": "3ddfbcffa14eaa69",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "# Load the lyrics embeddings datafarme\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df_lyrics_embeddings = pickle.load(file)\n",
    "\n",
    "def flatten_embeddings(embeddings):\n",
    "    return np.array(embeddings).flatten()\n",
    "\n",
    "df['lyrics_embeddings'] = df_lyrics_embeddings['lyrics_embeddings']\n",
    "df['lyrics_embeddings'] = df['lyrics_embeddings'].apply(flatten_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:20.878751Z",
     "start_time": "2024-05-31T10:42:20.855932Z"
    }
   },
   "id": "a17d812d4b942c6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "lyrics_features = np.vstack(df['lyrics_embeddings'].values)\n",
    "scaler_lyrics = StandardScaler()\n",
    "lyrics_features_normalized = scaler_lyrics.fit_transform(lyrics_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:23.616964Z",
     "start_time": "2024-05-31T10:42:23.599314Z"
    }
   },
   "id": "13aa7b098544e041",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "features = df[feature_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:24.516393Z",
     "start_time": "2024-05-31T10:42:24.508519Z"
    }
   },
   "id": "4832b31d5359fb6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "combined_features = np.hstack([features, lyrics_features_normalized])\n",
    "scaler_combined = StandardScaler()\n",
    "X_hardness_and_lyrics = scaler_combined.fit_transform(combined_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:28.754235Z",
     "start_time": "2024-05-31T10:42:28.733300Z"
    }
   },
   "id": "58cb1ca8b0d84360",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_hardness_and_lyrics = df['coast_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T10:42:29.765323Z",
     "start_time": "2024-05-31T10:42:29.755253Z"
    }
   },
   "id": "30bf705134a3e769",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "hardness_and_lyrics_metrics = run_ensemble_model(X_hardness_and_lyrics, y_hardness_and_lyrics, datatype='hardness_and_lyrics_and_topic_model', best_settings=best_model_settings, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:31:05.991906Z",
     "start_time": "2024-05-31T10:42:43.389507Z"
    }
   },
   "id": "58cbfe0da60fd833",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Hardness Features + Lyrics embeddings + Emotions</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a521e9ca788c25fc"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data_w_topics_w_features.xlsx', engine='openpyxl')\n",
    "\n",
    "df_w_emotions = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_emotion.pkl')\n",
    "\n",
    "indices_file_path = '/Users/borosabel/Documents/Uni/Thesis/PopMIR/Code/Lyrics/indexes.txt'\n",
    "\n",
    "# Read the file and convert each line to an integer\n",
    "with open(file_path, 'r') as file:\n",
    "    indices = [int(line.strip()) for line in file]\n",
    "\n",
    "df = df.iloc[indices]\n",
    "\n",
    "df_w_emotions = df_w_emotions.iloc[indices]\n",
    "\n",
    "columns_to_remove = ['Artist', 'Album', 'Song', 'Release Year', 'Tempo2', 'Duration (s)', 'Sample Rate (Hz)', 'Path', 'Lyrics', 'Coast']\n",
    "label_column = 'Coast'\n",
    "feature_columns = [item for item in list(df.columns) if item not in columns_to_remove]\n",
    "\n",
    "feeling_columns = ['joy', 'anger', 'sadness', 'fear', 'love', 'surprise']\n",
    "feature_columns.extend(feeling_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:35:32.499178Z",
     "start_time": "2024-05-31T14:35:28.925194Z"
    }
   },
   "id": "850a1151c785a426",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "df[feeling_columns] = df_w_emotions[feeling_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:35:32.527951Z",
     "start_time": "2024-05-31T14:35:32.524707Z"
    }
   },
   "id": "7cf6d82d69d0955a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "# Load the lyrics embeddings datafarme\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df_lyrics_embeddings = pickle.load(file)\n",
    "\n",
    "def flatten_embeddings(embeddings):\n",
    "    return np.array(embeddings).flatten()\n",
    "\n",
    "df['lyrics_embeddings'] = df_lyrics_embeddings['lyrics_embeddings']\n",
    "df['lyrics_embeddings'] = df['lyrics_embeddings'].apply(flatten_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:35:32.568796Z",
     "start_time": "2024-05-31T14:35:32.525520Z"
    }
   },
   "id": "7784b343a91ae2f5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "features = df[feature_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:35:32.613666Z",
     "start_time": "2024-05-31T14:35:32.599272Z"
    }
   },
   "id": "62b95508feb92298",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "combined_features = np.hstack([features, lyrics_features_normalized])\n",
    "scaler_combined = StandardScaler()\n",
    "X_emotion = scaler_combined.fit_transform(combined_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ad148a853708e8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_emotion = df['coast_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T12:56:37.016059Z",
     "start_time": "2024-05-31T12:56:37.006747Z"
    }
   },
   "id": "2a59af859301baee",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "hardness_lyrics_feeling_metrics = run_ensemble_model(X_emotion, y_emotion, datatype='audio_and_lyrics_and_emotions', best_settings=best_model_settings ,save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T13:28:38.861188Z",
     "start_time": "2024-05-31T12:58:52.417458Z"
    }
   },
   "id": "3f47ba1b36706344",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Hardness Features + Lyrics embeddings + Emotions + NER</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65dedd2a648eb819"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data_w_topics_w_features.xlsx', engine='openpyxl')\n",
    "\n",
    "df_w_emotions = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_emotion.pkl')\n",
    "df_w_entities = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_entity.pkl')\n",
    "\n",
    "indices_file_path = '/Users/borosabel/Documents/Uni/Thesis/PopMIR/Code/Lyrics/indexes.txt'\n",
    "\n",
    "# Read the file and convert each line to an integer\n",
    "with open(file_path, 'r') as file:\n",
    "    indices = [int(line.strip()) for line in file]\n",
    "\n",
    "df = df.iloc[indices]\n",
    "\n",
    "# Function to check for locations\n",
    "def check_location(entities, location):\n",
    "    return any(entity['entity'] == location and entity['type'] == 'LOC' for entity in entities)\n",
    "\n",
    "# Create one-hot encoded columns\n",
    "df_w_entities['New_York'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_location(x, 'New York') else 0)\n",
    "df_w_entities['California'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_location(x, 'California') else 0)\n",
    "\n",
    "# Define columns to remove and feature columns\n",
    "columns_to_remove = ['Artist', 'Album', 'Song', 'Release Year', 'Tempo2', 'Duration (s)', 'Sample Rate (Hz)', 'Path', 'Lyrics', 'Coast']\n",
    "label_column = 'Coast'\n",
    "feature_columns = [item for item in list(df.columns) if item not in columns_to_remove]\n",
    "\n",
    "feeling_columns = ['joy', 'anger', 'sadness', 'fear', 'love', 'surprise']\n",
    "feature_columns.extend(feeling_columns)\n",
    "locations_columns = ['New_York', 'California']\n",
    "feature_columns.extend(locations_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:39:45.810272Z",
     "start_time": "2024-05-31T14:39:41.848124Z"
    }
   },
   "id": "8a2ef4556340e529",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "df[feeling_columns] = df_w_emotions[feeling_columns]\n",
    "df[locations_columns] = df_w_entities[locations_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:39:45.813927Z",
     "start_time": "2024-05-31T14:39:45.808238Z"
    }
   },
   "id": "d84df45f660e6c9e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "# Load the lyrics embeddings datafarme\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df_lyrics_embeddings = pickle.load(file)\n",
    "\n",
    "def flatten_embeddings(embeddings):\n",
    "    return np.array(embeddings).flatten()\n",
    "\n",
    "df['lyrics_embeddings'] = df_lyrics_embeddings['lyrics_embeddings']\n",
    "df['lyrics_embeddings'] = df['lyrics_embeddings'].apply(flatten_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:39:48.027154Z",
     "start_time": "2024-05-31T14:39:47.969416Z"
    }
   },
   "id": "6404e524013182a4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "features = df[feature_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:40:10.441368Z",
     "start_time": "2024-05-31T14:40:10.420994Z"
    }
   },
   "id": "8938421218b91b4d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "combined_features = np.hstack([features, lyrics_features_normalized])\n",
    "scaler_combined = StandardScaler()\n",
    "X_entities = scaler_combined.fit_transform(combined_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:40:32.246554Z",
     "start_time": "2024-05-31T14:40:32.179924Z"
    }
   },
   "id": "82aa2f1a8bbca079",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_entities = df['coast_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T14:40:44.506026Z",
     "start_time": "2024-05-31T14:40:44.298466Z"
    }
   },
   "id": "79ae1c0d89117f4b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "hardness_lyrics_entities_metrics = run_ensemble_model(X_entities, y_entities, datatype='audio_and_lyrics_and_emotions_and_locations', save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T16:38:23.124570Z",
     "start_time": "2024-05-31T14:41:16.421504Z"
    }
   },
   "id": "fdfb34208ce434f9",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>Hardness Features + Lyrics embeddings + Emotions + NER + PER NER</b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2ff8c12f6ec901"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data_w_topics_w_features.xlsx', engine='openpyxl')\n",
    "\n",
    "df_w_emotions = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_emotion.pkl')\n",
    "df_w_entities = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_entity.pkl')\n",
    "\n",
    "indices_file_path = '/Users/borosabel/Documents/Uni/Thesis/PopMIR/Code/Lyrics/indexes.txt'\n",
    "\n",
    "# Read the file and convert each line to an integer\n",
    "with open(file_path, 'r') as file:\n",
    "    indices = [int(line.strip()) for line in file]\n",
    "\n",
    "df = df.iloc[indices]\n",
    "\n",
    "# Function to check for locations\n",
    "def check_location(entities, location):\n",
    "    return any(entity['entity'] == location and entity['type'] == 'LOC' for entity in entities)\n",
    "\n",
    "def check_pers(entities, pers):\n",
    "    return any(entity['entity'] == pers and entity['type'] == 'PER' for entity in entities)\n",
    "\n",
    "df_w_entities['God'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'God') else 0)\n",
    "df_w_entities['E'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'E') else 0)\n",
    "df_w_entities['Ice Cube'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Ice Cube') else 0)\n",
    "df_w_entities['Dr'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Dr') else 0)\n",
    "df_w_entities['Dre'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Dre') else 0)\n",
    "df_w_entities['Ren'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Ren') else 0)\n",
    "df_w_entities['Warren G'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Warren G') else 0)\n",
    "df_w_entities['Ice'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Ice') else 0)\n",
    "df_w_entities['Jay'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Jay') else 0)\n",
    "df_w_entities['Jack'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Jack') else 0)\n",
    "df_w_entities['Joe'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Joe') else 0)\n",
    "df_w_entities['Jimmy'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Jimmy') else 0)\n",
    "df_w_entities['Mary'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Mary') else 0)\n",
    "df_w_entities['Muhammad'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Muhammad') else 0)\n",
    "df_w_entities['Mike'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Mike') else 0)\n",
    "df_w_entities['Chuck'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Chuck') else 0)\n",
    "df_w_entities['Mike D'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Mike D') else 0)\n",
    "df_w_entities['Cool J'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_pers(x, 'Cool J') else 0)\n",
    "\n",
    "people_columns = ['God', 'E', 'Ice Cube', 'Dr', 'Dre', 'Ren', 'Warren G', 'Ice', 'Jay', 'Jack', 'Joe', 'Jimmy', 'Mary', 'Muhammad', 'Mike', 'Chuck', 'Mike D', 'Cool J']\n",
    "\n",
    "# Create one-hot encoded columns\n",
    "df_w_entities['New_York'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_location(x, 'New York') else 0)\n",
    "df_w_entities['California'] = df_w_entities['Named_Entities'].apply(lambda x: 1 if check_location(x, 'California') else 0)\n",
    "\n",
    "# Define columns to remove and feature columns\n",
    "columns_to_remove = ['Artist', 'Album', 'Song', 'Release Year', 'Tempo2', 'Duration (s)', 'Sample Rate (Hz)', 'Path', 'Lyrics', 'Coast']\n",
    "label_column = 'Coast'\n",
    "feature_columns = [item for item in list(df.columns) if item not in columns_to_remove]\n",
    "\n",
    "feeling_columns = ['joy', 'anger', 'sadness', 'fear', 'love', 'surprise']\n",
    "feature_columns.extend(feeling_columns)\n",
    "locations_columns = ['New_York', 'California']\n",
    "feature_columns.extend(locations_columns)\n",
    "feature_columns.extend(people_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:07:15.874538Z",
     "start_time": "2024-06-02T20:07:11.762128Z"
    }
   },
   "id": "879f48e44204d25f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "df[feeling_columns] = df_w_emotions[feeling_columns]\n",
    "df[locations_columns] = df_w_entities[locations_columns]\n",
    "df[people_columns] = df_w_entities[people_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:11:35.265734Z",
     "start_time": "2024-06-02T20:11:35.233589Z"
    }
   },
   "id": "9086b1e571763ec7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "# Load the lyrics embeddings datafarme\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df_lyrics_embeddings = pickle.load(file)\n",
    "\n",
    "def flatten_embeddings(embeddings):\n",
    "    return np.array(embeddings).flatten()\n",
    "\n",
    "df['lyrics_embeddings'] = df_lyrics_embeddings['lyrics_embeddings']\n",
    "df['lyrics_embeddings'] = df['lyrics_embeddings'].apply(flatten_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:11:46.669425Z",
     "start_time": "2024-06-02T20:11:46.631173Z"
    }
   },
   "id": "e3dc08e732603f7b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "features = df[feature_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:12:03.175320Z",
     "start_time": "2024-06-02T20:12:03.167956Z"
    }
   },
   "id": "16495b50ec08a331",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "combined_features = np.hstack([features, lyrics_features_normalized])\n",
    "scaler_combined = StandardScaler()\n",
    "X_entities = scaler_combined.fit_transform(combined_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:12:51.833390Z",
     "start_time": "2024-06-02T20:12:51.794257Z"
    }
   },
   "id": "6b8f11ba0283a217",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['coast_label'] = label_encoder.fit_transform(df['Coast'])\n",
    "y_entities = df['coast_label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T20:12:53.243583Z",
     "start_time": "2024-06-02T20:12:53.233979Z"
    }
   },
   "id": "39ee47da31fffef9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "hardness_lyrics_entities_metrics = run_ensemble_model(X_entities, y_entities, datatype='audio_and_lyrics_and_emotions_and_locations', save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T21:39:59.312814Z",
     "start_time": "2024-06-02T20:12:54.203390Z"
    }
   },
   "id": "237121b631c05a25",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T16:57:10.156321Z",
     "start_time": "2024-05-31T16:57:10.118668Z"
    }
   },
   "id": "ffcad97aff7f1e53",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "all_metrics = {\n",
    "    'Lyrics Only': only_lyrics_metrics,\n",
    "    'Audio Only': hardness_audio_features_metrics,\n",
    "    'Lyrics and Audiot and Topics (*)': hardness_and_lyrics_metrics,\n",
    "    '(*) and Emotions (**)': hardness_lyrics_feeling_metrics,\n",
    "    '(**) and Locations (***)': hardness_lyrics_entities_metrics \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T17:05:48.406369Z",
     "start_time": "2024-05-31T17:05:48.402334Z"
    }
   },
   "id": "712322b3ad3bf1b4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_metrics(all_metrics):\n",
    "    # Prepare data for plotting\n",
    "    setups = list(all_metrics.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    classifiers = list(all_metrics[setups[0]]['accuracy'].keys())\n",
    "\n",
    "    # Initialize subplots\n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(15, 20))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        data = []\n",
    "        for setup in setups:\n",
    "            for clf in classifiers:\n",
    "                mean_score = np.mean(all_metrics[setup][metric][clf])\n",
    "                data.append([setup, clf, mean_score])\n",
    "\n",
    "        df_plot = pd.DataFrame(data, columns=['Setup', 'Classifier', metric.capitalize()])\n",
    "\n",
    "        sns.barplot(x='Setup', y=metric.capitalize(), hue='Classifier', data=df_plot, ax=axes[i])\n",
    "        axes[i].set_title(f'{metric.capitalize()} Comparison')\n",
    "        axes[i].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(all_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T17:08:27.799876Z",
     "start_time": "2024-05-31T17:08:26.883984Z"
    }
   },
   "id": "3a52e5602e270de2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "def plot_voting_classifier_metrics(all_metrics):\n",
    "    # Prepare data for plotting\n",
    "    setups = list(all_metrics.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    classifier = 'Voting Classifier'\n",
    "\n",
    "    data = []\n",
    "    for setup in setups:\n",
    "        for metric in metrics:\n",
    "            mean_score = np.mean(all_metrics[setup][metric][classifier])\n",
    "            data.append([setup, metric, mean_score])\n",
    "\n",
    "    df_plot = pd.DataFrame(data, columns=['Setup', 'Metric', 'Mean Score'])\n",
    "\n",
    "    # Create the line plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(x='Setup', y='Mean Score', hue='Metric', data=df_plot, marker='o')\n",
    "\n",
    "    plt.title('Voting Classifier Metrics Across Different Setups')\n",
    "    plt.ylabel('Mean Score')\n",
    "    plt.xlabel('Setup')\n",
    "    plt.legend(title='Metric', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Voting_classifier_metrics')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T19:34:02.946679Z",
     "start_time": "2024-06-02T19:34:02.924342Z"
    }
   },
   "id": "5b438447e755e888",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "plot_voting_classifier_metrics(all_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T19:34:03.523221Z",
     "start_time": "2024-06-02T19:34:03.258523Z"
    }
   },
   "id": "dbcb9ebd47e757ed",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63b9b2c07f431b77",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
