{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:47:22.999043Z",
     "start_time": "2024-06-10T17:47:16.008788Z"
    }
   },
   "source": [
    "import string\n",
    "import spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "import numpy as np\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import utility_functions as utils\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "custom_stop_words = list(STOP_WORDS)  # Existing stop words\n",
    "custom_stop_words.extend([\"ll\", \"ve\", \"'em\", \"em\", \"ho\", \"fo\", \"ah\", \"de\"])  # Tokens which doesn't really make sense to hav`e them."
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_w_topics_w_entity.xlsx', engine='openpyxl')\n",
    "_df = pd.read_excel('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/baseline_data.xlsx', engine='openpyxl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:48:02.097770Z",
     "start_time": "2024-06-10T17:48:01.700850Z"
    }
   },
   "id": "72d5586ddc206c07",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "df['Lyrics'] = _df['Lyrics'].apply(utils.cleanup_entity_rec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T10:49:53.954242Z",
     "start_time": "2024-06-10T10:49:53.260869Z"
    }
   },
   "id": "10e3e1af7638a0a9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "# Initialize the NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    return [{'entity': entity['word'], 'type': entity['entity_group'], 'score': entity['score']} for entity in entities]\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting entities\")\n",
    "df['Named_Entities'] = df['Lyrics'].progress_apply(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T11:48:20.928965Z",
     "start_time": "2024-06-10T10:49:53.956404Z"
    }
   },
   "id": "2acade43f4d0f7e8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "city_to_state_mapping = {\n",
    "    'New York': ['Bronx', 'Brooklyn', 'Coney Island', 'Harlem', 'Manhattan', 'Queens', 'Staten Island', 'Upper East Side', 'Bed-Stuy', 'Bedford-Stuyvesant', 'Bushwick', 'Kings County', 'Williamsburg', 'East Village', 'Fifth Avenue', 'Lower East Side', 'Meatpacking District', 'SoHo', 'Tribeca', 'Union Square', 'West Village'],\n",
    "    'California': ['Los Angeles', 'San Francisco', 'Oakland', 'Sacramento', 'San Diego', 'San Jose', 'Fresno', 'Santa Monica', 'Hollywood', 'Beverly Hills', 'Long Beach', 'Malibu', 'Napa Valley', 'Palm Springs', 'Santa Barbara', 'Silicon Valley', 'Venice Beach', 'West Hollywood'],\n",
    "    'Texas': ['Austin', 'Dallas', 'Houston', 'San Antonio', 'Fort Worth', 'El Paso'],\n",
    "    'Florida': ['Miami', 'Orlando', 'Tampa', 'Jacksonville', 'Fort Lauderdale', 'Tallahassee', 'West Palm Beach'],\n",
    "    'Illinois': ['Chicago'],\n",
    "    'Georgia': ['Atlanta', 'Savannah'],\n",
    "    'New Jersey': ['Newark', 'Jersey City', 'Hoboken', 'Atlantic City', 'Paterson'],\n",
    "    'Pennsylvania': ['Philadelphia', 'Pittsburgh', 'Harrisburg'],\n",
    "    'Massachusetts': ['Boston', 'Cambridge', 'Worcester', 'Springfield'],\n",
    "    'Nevada': ['Las Vegas', 'Reno'],\n",
    "    'Washington': ['Seattle', 'Spokane', 'Tacoma'],\n",
    "    'Arizona': ['Phoenix', 'Tucson'],\n",
    "    'Michigan': ['Detroit', 'Ann Arbor'],\n",
    "    'Ohio': ['Cleveland', 'Cincinnati', 'Columbus', 'Akron'],\n",
    "    'North Carolina': ['Charlotte', 'Raleigh', 'Durham'],\n",
    "    'South Carolina': ['Charleston', 'Columbia'],\n",
    "    'Virginia': ['Richmond', 'Virginia Beach', 'Arlington', 'Norfolk'],\n",
    "    'Maryland': ['Baltimore', 'Annapolis'],\n",
    "    'Colorado': ['Denver', 'Colorado Springs'],\n",
    "    'Louisiana': ['New Orleans', 'Baton Rouge'],\n",
    "    'Alabama': ['Birmingham', 'Montgomery'],\n",
    "    'Tennessee': ['Nashville', 'Memphis'],\n",
    "    'Kentucky': ['Louisville', 'Lexington'],\n",
    "    'Missouri': ['St. Louis', 'Kansas City'],\n",
    "    'Minnesota': ['Minneapolis', 'St. Paul'],\n",
    "    'Oregon': ['Portland', 'Eugene'],\n",
    "    'Indiana': ['Indianapolis', 'Fort Wayne'],\n",
    "    'Iowa': ['Des Moines'],\n",
    "    'Kansas': ['Wichita'],\n",
    "    'Nebraska': ['Omaha', 'Lincoln'],\n",
    "    'New Mexico': ['Albuquerque'],\n",
    "    'Oklahoma': ['Oklahoma City', 'Tulsa'],\n",
    "    'Arkansas': ['Little Rock'],\n",
    "    'Connecticut': ['Hartford', 'New Haven'],\n",
    "    'Rhode Island': ['Providence'],\n",
    "    'Maine': ['Portland', 'Augusta'],\n",
    "    'New Hampshire': ['Manchester', 'Concord'],\n",
    "    'Vermont': ['Burlington', 'Montpelier'],\n",
    "    'Idaho': ['Boise'],\n",
    "    'Montana': ['Billings'],\n",
    "    'Wyoming': ['Cheyenne'],\n",
    "    'South Dakota': ['Sioux Falls'],\n",
    "    'North Dakota': ['Fargo'],\n",
    "    'West Virginia': ['Charleston']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:53:24.380319Z",
     "start_time": "2024-06-10T17:53:24.373490Z"
    }
   },
   "id": "e9b5c53b98387539",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df.to_pickle('df_w_entity_w_emotions.pkl')\n",
    "df = pd.read_pickle('/Users/borosabel/Documents/Uni/Thesis/PopMIR/Data/Excel/df_w_entity_w_emotions.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:48:15.882816Z",
     "start_time": "2024-06-10T17:48:15.868357Z"
    }
   },
   "id": "c4e908922976f2eb",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "with open('corpus_entity.txt', 'w', encoding='utf-8') as file:\n",
    "    for lyrics in df['Lyrics']:\n",
    "        if pd.notna(lyrics):\n",
    "            file.write(lyrics + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T20:59:52.316090Z",
     "start_time": "2024-06-09T20:59:52.307400Z"
    }
   },
   "id": "eac35b30bf74e951",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "topics0 = df[df['Topic Model Index'] == 0]['Topic Model'].iloc[0]\n",
    "topics1 = df[df['Topic Model Index'] == 1]['Topic Model'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:48:58.014684Z",
     "start_time": "2024-06-10T17:48:58.005295Z"
    }
   },
   "id": "240fd11091e903c9",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T17:49:01.880133Z",
     "start_time": "2024-06-10T17:49:01.855162Z"
    }
   },
   "id": "c495e7ab1db8220b",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_east = df[df['Coast'] == 'east_coast'].reset_index(drop=True)\n",
    "df_west = df[df['Coast'] == 'west_coast'].reset_index(drop=True)\n",
    "\n",
    "df_east_1 = df_east[df_east['Topic Model Index'] == 0].reset_index(drop=True)\n",
    "df_east_2 = df_east[df_east['Topic Model Index'] == 1].reset_index(drop=True)\n",
    "# df_east_3 = df_east[df_east['Topic Model Index'] == 2]\n",
    "# df_east_4 = df_east[df_east['Topic Model Index'] == 3]\n",
    "\n",
    "df_west_1 = df_west[df_west['Topic Model Index'] == 0].reset_index(drop=True)\n",
    "df_west_2 = df_west[df_west['Topic Model Index'] == 1].reset_index(drop=True)\n",
    "# df_west_3 = df_west[df_west['Topic Model Index'] == 2]\n",
    "# df_west_4 = df_west[df_west['Topic Model Index'] == 3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:09.170422Z",
     "start_time": "2024-06-10T22:33:09.135773Z"
    }
   },
   "id": "ba848957add01f06",
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def populate_entities_by_type(df):\n",
    "    entities_by_type = defaultdict(list)\n",
    "    entity_indices = defaultdict(list)\n",
    "\n",
    "    for index, (entities, artist) in enumerate(zip(df['Named_Entities'], df['Artist'])):\n",
    "        if isinstance(entities, list):\n",
    "            for entity in entities:\n",
    "                if entity['score'] > 0.7:\n",
    "                    mapped_entity = map_city_to_state(entity['entity'], city_to_state_mapping)\n",
    "                    entities_by_type[entity['type']].append(mapped_entity)\n",
    "                    entity_indices[(mapped_entity, artist)].append(index)\n",
    "\n",
    "    return entities_by_type, entity_indices\n",
    "\n",
    "def find_entries_containing_entities(df, entity_indices, search_entities):\n",
    "    # Collect indices of rows that contain the search entities\n",
    "    matching_indices = set()\n",
    "    for entity in search_entities:\n",
    "        for (ent, artist), indices in entity_indices.items():\n",
    "            if ent == entity:\n",
    "                matching_indices.update(indices)\n",
    "\n",
    "    # Ensure matching_indices are valid\n",
    "    matching_indices = list(matching_indices)\n",
    "    matching_indices = [idx for idx in matching_indices if idx in df.index]\n",
    "\n",
    "    # Create a subset of the DataFrame with the matching rows\n",
    "    subset_df = df.loc[matching_indices]\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "def extract_context_from_lyrics(df, entity, window=3):\n",
    "    context_info = []\n",
    "\n",
    "    # Iterate through the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        lyrics = row['Lyrics']\n",
    "        song_name = row['Song']\n",
    "        artist = row['Artist']\n",
    "        # Tokenize the lyrics into words\n",
    "        words = lyrics.split()\n",
    "        # Find the entity in the words list and extract context\n",
    "        for i, word in enumerate(words):\n",
    "            if word == entity:\n",
    "                start = max(i - window, 0)\n",
    "                end = min(i + window + 1, len(words))\n",
    "                context = words[start:end]\n",
    "                context_info.append({\n",
    "                    'Song_Name': song_name,\n",
    "                    'Artist': artist,\n",
    "                    'Context': ' '.join(context)\n",
    "                })\n",
    "\n",
    "    return context_info\n",
    "\n",
    "def postprocess_mapping(entities_by_type, entity_indices, mapping):\n",
    "    updated_entities_by_type = defaultdict(list)\n",
    "    updated_entity_indices = defaultdict(list)\n",
    "\n",
    "    for entity_type, entities in entities_by_type.items():\n",
    "        for entity in entities:\n",
    "            new_entity = entity\n",
    "            if entity in mapping.get(entity_type, {}):\n",
    "                for (ent, artist), indices in entity_indices.items():\n",
    "                    if ent == entity and artist in mapping[entity_type][entity]:\n",
    "                        new_entity = mapping[entity_type][entity][artist]\n",
    "                        break\n",
    "            updated_entities_by_type[entity_type].append(new_entity)\n",
    "\n",
    "            for (ent, artist), indices in entity_indices.items():\n",
    "                if ent == entity:\n",
    "                    updated_entity_indices[(new_entity, artist)].extend(indices)\n",
    "\n",
    "    return updated_entities_by_type, updated_entity_indices\n",
    "\n",
    "def map_city_to_state(entity, city_to_state_mapping):\n",
    "    for state, cities in city_to_state_mapping.items():\n",
    "        if entity in cities:\n",
    "            return state\n",
    "    return entity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:36:31.426418Z",
     "start_time": "2024-06-10T22:36:31.415342Z"
    }
   },
   "id": "5b581321bfc71993",
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "entities_by_type_east, entity_indices_east = populate_entities_by_type(df_east)\n",
    "entities_by_type_east_1, entity_indices_east_1 = populate_entities_by_type(df_east_1)\n",
    "entities_by_type_east_2, entity_indices_east_2 = populate_entities_by_type(df_east_2)\n",
    "# entities_by_type_east_3 = populate_entities_by_type(df_east_3)\n",
    "# entities_by_type_east_4 = populate_entities_by_type(df_east_4)\n",
    "\n",
    "entities_by_type_west, entity_indices_west = populate_entities_by_type(df_west)\n",
    "entities_by_type_west_1, entity_indices_west_1 = populate_entities_by_type(df_west_1)\n",
    "entities_by_type_west_2, entity_indices_west_2 = populate_entities_by_type(df_west_2)\n",
    "# entities_by_type_west_3 = populate_entities_by_type(df_west_3)\n",
    "# entities_by_type_west_4 = populate_entities_by_type(df_west_4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:10.656877Z",
     "start_time": "2024-06-10T22:33:10.601531Z"
    }
   },
   "id": "bfc9945962934d47",
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Since there are a lot of entities with only one letter let's figure out what the fuck are they?\n",
    "entities_by_type, entity_indices = populate_entities_by_type(df_east)\n",
    "entity_to_search = 'York'\n",
    "subset_df = find_entries_containing_entities(df_east, entity_indices, [entity_to_search])\n",
    "context_words = extract_context_from_lyrics(subset_df, entity_to_search, window=5)\n",
    "\n",
    "for context in context_words:\n",
    "    print(f\"Context for entity '{entity_to_search}': {context}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:12.487447Z",
     "start_time": "2024-06-10T22:33:12.466410Z"
    }
   },
   "id": "20e0574a0a032",
   "execution_count": 77,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T22:39:40.985784Z",
     "start_time": "2024-06-10T22:39:40.934282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since there are a lot of entities with only one letter let's figure out what the fuck are they?\n",
    "entities_by_type, entity_indices = populate_entities_by_type(df_west)\n",
    "entity_to_search = 'York'\n",
    "subset_df = find_entries_containing_entities(df_west, entity_indices, [entity_to_search])\n",
    "context_words = extract_context_from_lyrics(subset_df, entity_to_search, window=5)\n",
    "\n",
    "for context in context_words:\n",
    "    print(f\"Context for entity '{entity_to_search}': {context}\")"
   ],
   "id": "2bb48cb96e3095da",
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "entity_mapping_east = {\n",
    "    'PER': {\n",
    "        'D': {\n",
    "            'Run-DMC': 'D.M.C.',\n",
    "            'Beastie Boys': 'Mike D',\n",
    "            'LL cool J': 'L.O.D.'\n",
    "        },\n",
    "        'Jay': {\n",
    "            'Run-DMC': 'Jam Master Jay',\n",
    "            'The Notorious B.I.G': 'Jay-Z',\n",
    "            'Jay-Z': 'Jay-Z',\n",
    "            'Nas': 'Jay-Z'\n",
    "        },\n",
    "        'Mike': {\n",
    "            'Beastie Boys': 'Mike D',\n",
    "        }\n",
    "    },\n",
    "    'LOC': {\n",
    "        'York': {\n",
    "            'Public Enemy': 'New York',\n",
    "            'Wu Tang Clan': 'New York',\n",
    "            'Mobb Deep': 'New York',\n",
    "            'Beastie Boys': 'New York',\n",
    "            'Gang Starr': 'New York',\n",
    "            'Eric B and Rakim': 'New York'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "entity_mapping_west = {\n",
    "    'PER': {\n",
    "        'D': {\n",
    "            'Run-DMC': 'D.M.C.',\n",
    "            'Beastie Boys': 'Mike D',\n",
    "            'LL cool J': 'L.O.D.'\n",
    "        },\n",
    "        'Jay': {\n",
    "            'Run-DMC': 'Jam Master Jay',\n",
    "            'The Notorious B.I.G': 'Jay-Z',\n",
    "            'Jay-Z': 'Jay-Z',\n",
    "            'Nas': 'Jay-Z'\n",
    "        },\n",
    "        'Mike': {\n",
    "            'Beastie Boys': 'Mike D',\n",
    "        }\n",
    "    },\n",
    "    'LOC': {\n",
    "        'York': {\n",
    "            'Public Enemy': 'New York',\n",
    "            'Wu Tang Clan': 'New York',\n",
    "            'Mobb Deep': 'New York',\n",
    "            'Beastie Boys': 'New York',\n",
    "            'Gang Starr': 'New York',\n",
    "            'Eric B and Rakim': 'New York'\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:19.080297Z",
     "start_time": "2024-06-10T22:33:19.075658Z"
    }
   },
   "id": "ad4193bf42f8c1e0",
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply postprocessing mapping\n",
    "entities_by_type_east, entity_indices_east = postprocess_mapping(entities_by_type_east, entity_indices_east, entity_mapping_east)\n",
    "entities_by_type_east, entity_indices_east = postprocess_mapping(entities_by_type_east, entity_indices_east, entity_mapping_east)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:24:12.768398Z",
     "start_time": "2024-06-10T22:24:12.366442Z"
    }
   },
   "id": "cb32ac9cbe521f28",
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def most_common_entities_by_type(entities_by_type):\n",
    "    most_common_by_type = {}\n",
    "    for entity_type, entities in entities_by_type.items():\n",
    "        entity_counter = Counter(entities)\n",
    "        most_common_by_type[entity_type] = entity_counter.most_common(10)\n",
    "    return most_common_by_type\n",
    "\n",
    "def print_most_common_entities(most_common_by_type, label):\n",
    "    print(f\"Most common entities for {label}:\")\n",
    "    for entity_type, common_entities in most_common_by_type.items():\n",
    "        print(f\"Top 10 common entities for type {entity_type}:\")\n",
    "        for entity, count in common_entities:\n",
    "            print(f\"{entity}: {count}\")\n",
    "    print()\n",
    "\n",
    "most_common_by_type_east = most_common_entities_by_type(entities_by_type_east)\n",
    "most_common_by_type_east_1 = most_common_entities_by_type(entities_by_type_east_1)\n",
    "most_common_by_type_east_2 = most_common_entities_by_type(entities_by_type_east_2)\n",
    "# most_common_by_type_east_3 = most_common_entities_by_type(entities_by_type_east_3)\n",
    "# most_common_by_type_east_4 = most_common_entities_by_type(entities_by_type_east_4)\n",
    "\n",
    "most_common_by_type_west = most_common_entities_by_type(entities_by_type_west)\n",
    "most_common_by_type_west_1 = most_common_entities_by_type(entities_by_type_west_1)\n",
    "most_common_by_type_west_2 = most_common_entities_by_type(entities_by_type_west_2)\n",
    "# most_common_by_type_west_3 = most_common_entities_by_type(entities_by_type_west_3)\n",
    "# most_common_by_type_west_4 = most_common_entities_by_type(entities_by_type_west_4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:21.533694Z",
     "start_time": "2024-06-10T22:33:21.524812Z"
    }
   },
   "id": "774533b5da9febea",
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print_most_common_entities(most_common_by_type_east, 'East Coast')\n",
    "print_most_common_entities(most_common_by_type_east_1, f'East Coast, Topic Model {topics0}')\n",
    "print_most_common_entities(most_common_by_type_east_2, f'East Coast, Topic Model {topics1}')\n",
    "# print_most_common_entities(most_common_by_type_east_3, f'East Coast, Topic Model {topics2}')\n",
    "# print_most_common_entities(most_common_by_type_east_4, f'East Coast, Topic Model {topics3}')\n",
    "\n",
    "print_most_common_entities(most_common_by_type_west, 'West Coast')\n",
    "print_most_common_entities(most_common_by_type_west_1, f'West Coast, Topic Model {topics0}')\n",
    "print_most_common_entities(most_common_by_type_west_2, f'West Coast, Topic Model {topics1}')\n",
    "# print_most_common_entities(most_common_by_type_west_3, f'West Coast, Topic Model {topics2}')\n",
    "# print_most_common_entities(most_common_by_type_west_4, f'West Coast, Topic Model {topics3}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:22.663699Z",
     "start_time": "2024-06-10T22:33:22.658790Z"
    }
   },
   "id": "997292b36178262f",
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_connected_infos(df, entity_indices, search_entities, plot_titles, file_names):\n",
    "    # Find entries containing the search entities\n",
    "    subset_df = find_entries_containing_entities(df, entity_indices, search_entities)\n",
    "    print(len(subset_df))\n",
    "    # Filter subsets by Topic Model Index\n",
    "    subset_df_topic_0 = subset_df[subset_df['Topic Model Index'] == 0]\n",
    "    subset_df_topic_1 = subset_df[subset_df['Topic Model Index'] == 1]\n",
    "\n",
    "    # Plot the percentage of Topic Model Index\n",
    "    topic_model_counts = subset_df['Topic Model Index'].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Define colors based on Topic Model Index\n",
    "    colors = {0: 'blue', 1: 'red'}\n",
    "    color_list = [colors[index] for index in topic_model_counts.index]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = topic_model_counts.plot(kind='bar', color=color_list)\n",
    "    plt.title(plot_titles[0])\n",
    "    plt.xlabel('Topic Model Index')\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "    # Annotate bars with percentages\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}%',\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10),\n",
    "                    textcoords='offset points')\n",
    "    plt.savefig(file_names[0])\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and plot the average values of emotions for subset_df_topic_0\n",
    "    emotion_columns = ['joy', 'anger', 'sadness', 'fear', 'love', 'surprise']\n",
    "    avg_emotions_topic_0 = subset_df_topic_0[emotion_columns].mean() * 100\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = avg_emotions_topic_0.plot(kind='bar', color='blue')\n",
    "    plt.title(plot_titles[1])\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Average Value (%)')\n",
    "\n",
    "    # Annotate bars with average values\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}%',\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10),\n",
    "                    textcoords='offset points')\n",
    "    plt.savefig(file_names[1])\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and plot the average values of emotions for subset_df_topic_1\n",
    "    avg_emotions_topic_1 = subset_df_topic_1[emotion_columns].mean() * 100\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = avg_emotions_topic_1.plot(kind='bar', color='red')\n",
    "    plt.title(plot_titles[2])\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Average Value (%)')\n",
    "\n",
    "    # Annotate bars with average values\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}%',\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10),\n",
    "                    textcoords='offset points')\n",
    "    plt.savefig(file_names[2])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:40:01.748245Z",
     "start_time": "2024-06-10T22:40:01.734165Z"
    }
   },
   "id": "c70d90db10e556bc",
   "execution_count": 92,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "entity = ['California']\n",
    "plot_connected_infos(\n",
    "    df_east,\n",
    "    entity_indices_east,\n",
    "    entity,\n",
    "    [\n",
    "        f'Percentage of Topics/East Coast/{entity[0]}',\n",
    "        f'Percentage of Emotions/East Coast/{entity[0]}/Topic 0',\n",
    "        f'Percentage of Emotions/East Coast/{entity[0]}/Topic 1'\n",
    "    ],\n",
    "    [\n",
    "        f'topics_east_coast_{entity[0]}',\n",
    "        f'topics_east_coast_{entity[0]}_topic_0',\n",
    "        f'topics_east_coast_{entity[0]}_topic_1'\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T22:40:13.187578Z",
     "start_time": "2024-06-10T22:40:12.632664Z"
    }
   },
   "id": "ba7092eda76f5b9d",
   "execution_count": 94,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T22:41:21.711349Z",
     "start_time": "2024-06-10T22:41:21.173707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "entity = ['California']\n",
    "plot_connected_infos(\n",
    "    df_west,\n",
    "    entity_indices_west,\n",
    "    entity,\n",
    "    [\n",
    "        f'Percentage of Topics/West Coast/{entity[0]}',\n",
    "        f'Percentage of Emotions/West Coast/{entity[0]}/Topic 0',\n",
    "        f'Percentage of Emotions/West Coast/{entity[0]}/Topic 1'\n",
    "    ],\n",
    "    [\n",
    "        f'topics_west_coast_{entity[0]}',\n",
    "        f'topics_west_coast_{entity[0]}_topic_0',\n",
    "        f'topics_west_coast_{entity[0]}_topic_1'\n",
    "    ]\n",
    ")"
   ],
   "id": "dc5e5b6618021955",
   "execution_count": 96,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T06:50:46.003508Z",
     "start_time": "2024-06-11T06:50:45.976875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_combined_infos(df_east, df_west, entity_indices_east, entity_indices_west, search_entities, plot_titles, file_names):\n",
    "    # Find entries containing the search entities in both dataframes\n",
    "    subset_df_east = find_entries_containing_entities(df_east, entity_indices_east, search_entities)\n",
    "    subset_df_west = find_entries_containing_entities(df_west, entity_indices_west, search_entities)\n",
    "    emotion_columns = ['joy', 'anger', 'sadness', 'fear', 'love', 'surprise']\n",
    "\n",
    "    # Combine East and West dataframes\n",
    "    subset_df_east['Region'] = 'East Coast'\n",
    "    subset_df_west['Region'] = 'West Coast'\n",
    "    combined_df = pd.concat([subset_df_east, subset_df_west])\n",
    "\n",
    "    # Plot the percentage of Topic Model Index for both regions\n",
    "    topic_model_counts_east = subset_df_east['Topic Model Index'].value_counts(normalize=True) * 100\n",
    "    topic_model_counts_west = subset_df_west['Topic Model Index'].value_counts(normalize=True) * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = topic_model_counts_east.index\n",
    "\n",
    "    ax.bar(index, topic_model_counts_east, bar_width, label='East Coast', color='red')\n",
    "    ax.bar(index + bar_width, topic_model_counts_west, bar_width, label='West Coast', color='blue')\n",
    "\n",
    "    ax.set_xlabel('Topic Model Index')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title(plot_titles[0])\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(index)\n",
    "    ax.legend()\n",
    "\n",
    "    for i, v in enumerate(topic_model_counts_east):\n",
    "        ax.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    for i, v in enumerate(topic_model_counts_west):\n",
    "        ax.text(i + bar_width, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "    plt.savefig(file_names[0])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the average values of emotions for both Topic Model Index 0\n",
    "    subset_df_topic_0_east = subset_df_east[subset_df_east['Topic Model Index'] == 0]\n",
    "    subset_df_topic_0_west = subset_df_west[subset_df_west['Topic Model Index'] == 0]\n",
    "\n",
    "    avg_emotions_topic_0_east = subset_df_topic_0_east[emotion_columns].mean() * 100\n",
    "    avg_emotions_topic_0_west = subset_df_topic_0_west[emotion_columns].mean() * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    index = range(len(emotion_columns))\n",
    "\n",
    "    ax.bar(index, avg_emotions_topic_0_east, bar_width, label='East Coast', color='red')\n",
    "    ax.bar([i + bar_width for i in index], avg_emotions_topic_0_west, bar_width, label='West Coast', color='blue')\n",
    "\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Average Value (%)')\n",
    "    ax.set_title(plot_titles[1])\n",
    "    ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "    ax.set_xticklabels(emotion_columns)\n",
    "    ax.legend()\n",
    "\n",
    "    for i, v in enumerate(avg_emotions_topic_0_east):\n",
    "        ax.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    for i, v in enumerate(avg_emotions_topic_0_west):\n",
    "        ax.text(i + bar_width, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "    plt.savefig(file_names[1])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the average values of emotions for both Topic Model Index 1\n",
    "    subset_df_topic_1_east = subset_df_east[subset_df_east['Topic Model Index'] == 1]\n",
    "    subset_df_topic_1_west = subset_df_west[subset_df_west['Topic Model Index'] == 1]\n",
    "\n",
    "    avg_emotions_topic_1_east = subset_df_topic_1_east[emotion_columns].mean() * 100\n",
    "    avg_emotions_topic_1_west = subset_df_topic_1_west[emotion_columns].mean() * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    index = range(len(emotion_columns))\n",
    "\n",
    "    ax.bar(index, avg_emotions_topic_1_east, bar_width, label='East Coast', color='red')\n",
    "    ax.bar([i + bar_width for i in index], avg_emotions_topic_1_west, bar_width, label='West Coast', color='blue')\n",
    "\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Average Value (%)')\n",
    "    ax.set_title(plot_titles[2])\n",
    "    ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "    ax.set_xticklabels(emotion_columns)\n",
    "    ax.legend()\n",
    "\n",
    "    for i, v in enumerate(avg_emotions_topic_1_east):\n",
    "        ax.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "    for i, v in enumerate(avg_emotions_topic_1_west):\n",
    "        ax.text(i + bar_width, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "    plt.savefig(file_names[2])\n",
    "    plt.show()"
   ],
   "id": "dd095197baae9466",
   "execution_count": 106,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T06:55:09.763635Z",
     "start_time": "2024-06-11T06:55:09.195384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "entity = ['California']\n",
    "plot_combined_infos(\n",
    "    df_east,\n",
    "    df_west,\n",
    "    entity_indices_east,\n",
    "    entity_indices_west,\n",
    "    entity,\n",
    "    [\n",
    "        f'Percentage of Topics/East_West Coast/{entity[0]}',\n",
    "        f'Percentage of Emotions/East_West Coast/{entity[0]}/Topic 0',\n",
    "        f'Percentage of Emotions/East_West Coast/{entity[0]}/Topic 1'\n",
    "    ],\n",
    "    [\n",
    "        f'topics_east_west_coast_{entity[0]}',\n",
    "        f'topics_east_west_coast_{entity[0]}_topic_0',\n",
    "        f'topics_east_west_coast_{entity[0]}_topic_1'\n",
    "    ]\n",
    ")"
   ],
   "id": "f3c9b14ff48579d5",
   "execution_count": 109,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "id": "d5bd813edf6c8189",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "def plot_top_person_entities(most_common_east, most_common_west):\n",
    "    # Extract top person entities and their counts\n",
    "    top_persons_east = {entity: count for entity, count in most_common_east.get('PER', [])[:1]}\n",
    "    top_persons_west = {entity: count for entity, count in most_common_west.get('PER', [])[:1]}\n",
    "\n",
    "    # Calculate total counts for each coast\n",
    "    total_east = sum(count for entity, count in most_common_east.get('PER', []))\n",
    "    total_west = sum(count for entity, count in most_common_west.get('PER', []))\n",
    "\n",
    "    # Combine the data into a DataFrame for plotting\n",
    "    data = []\n",
    "    for entity, count in top_persons_east.items():\n",
    "        percentage = (count / total_east) * 100 if total_east > 0 else 0\n",
    "        data.append({'Entity': entity, 'Count': count, 'Percentage': percentage, 'Coast': 'East'})\n",
    "    for entity, count in top_persons_west.items():\n",
    "        percentage = (count / total_west) * 100 if total_west > 0 else 0\n",
    "        data.append({'Entity': entity, 'Count': count, 'Percentage': percentage, 'Coast': 'West'})\n",
    "\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x='Entity', y='Count', hue='Coast', data=df_plot, palette={'East': 'blue', 'West': 'red'})\n",
    "    plt.title('Most Popular Person Entity in East Coast and West Coast')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add count and percentage labels on top of the bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}',\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T20:15:58.667241Z",
     "start_time": "2024-05-15T20:15:58.663793Z"
    }
   },
   "id": "78ced9b47a0a2c7b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "plot_top_person_entities(most_common_by_type_east, most_common_by_type_west)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T20:15:59.300320Z",
     "start_time": "2024-05-15T20:15:59.218257Z"
    }
   },
   "id": "207fb0fa88e086b3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def plot_top_location_entities(most_common_east, most_common_west):\n",
    "    # Extract top person entities and their counts\n",
    "    top_persons_east = {entity: count for entity, count in most_common_east.get('LOC', [])[:10]}\n",
    "    top_persons_west = {entity: count for entity, count in most_common_west.get('LOC', [])[:10]}\n",
    "\n",
    "    # Calculate total counts for each coast\n",
    "    total_east = sum(count for entity, count in most_common_east.get('LOC', []))\n",
    "    total_west = sum(count for entity, count in most_common_west.get('LOC', []))\n",
    "\n",
    "    # Combine the data into a DataFrame for plotting\n",
    "    data = []\n",
    "    for entity, count in top_persons_east.items():\n",
    "        percentage = (count / total_east) * 100 if total_east > 0 else 0\n",
    "        data.append({'Entity': entity, 'Count': count, 'Percentage': percentage, 'Coast': 'East'})\n",
    "    for entity, count in top_persons_west.items():\n",
    "        percentage = (count / total_west) * 100 if total_west > 0 else 0\n",
    "        data.append({'Entity': entity, 'Count': count, 'Percentage': percentage, 'Coast': 'West'})\n",
    "\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x='Entity', y='Count', hue='Coast', data=df_plot, palette={'East': 'blue', 'West': 'red'})\n",
    "    plt.title('Most Popular Person Entity in East Coast and West Coast')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add count and percentage labels on top of the bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}',\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:34:49.952746Z",
     "start_time": "2024-05-17T10:34:49.951493Z"
    }
   },
   "id": "574db8e7cfc601e8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "plot_top_location_entities(most_common_by_type_east, most_common_by_type_west)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:34:51.268537Z",
     "start_time": "2024-05-17T10:34:51.084754Z"
    }
   },
   "id": "8085aa148ac55dad",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def populate_locations_by_coast(df):\n",
    "    locations_by_coast = defaultdict(list)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['Named_Entities'], list):\n",
    "            for entity in row['Named_Entities']:\n",
    "                if entity['type'] == 'LOC' and entity['score'] > 0.5:  # Assuming 'LOC' is the type for locations\n",
    "                    locations_by_coast[row['Coast']].append(entity['entity'])\n",
    "\n",
    "    return locations_by_coast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T17:17:09.470316Z",
     "start_time": "2024-05-17T17:17:09.467353Z"
    }
   },
   "id": "f2c63ffaecace20f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "locations_by_coast = populate_locations_by_coast(df)\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier handling\n",
    "location_data = []\n",
    "for coast, locations in locations_by_coast.items():\n",
    "    for location in locations:\n",
    "            location_data.append({'Coast': coast, 'Location': location})\n",
    "\n",
    "location_df = pd.DataFrame(location_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T17:17:15.264492Z",
     "start_time": "2024-05-17T17:17:15.212413Z"
    }
   },
   "id": "81ff3a88210da4a2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "location_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T17:17:16.349452Z",
     "start_time": "2024-05-17T17:17:16.340948Z"
    }
   },
   "id": "4d6a8ed37d6c4282",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "location_df.to_excel('locations_by_coast.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T17:17:33.974418Z",
     "start_time": "2024-05-17T17:17:33.905081Z"
    }
   },
   "id": "6be377c47f65a5b8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "68bdfc3b6c5eb133",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
